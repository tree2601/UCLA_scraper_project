# -*- coding: utf-8 -*-
"""wst_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rfn1eUgo2ukQAe8S15Y0T-H7JSGbIIHP
"""

!pip install bs4
import urllib.request
from bs4 import BeautifulSoup
import time
import re
import heapq
http = 'https://www.ucla.edu/'

def crawl(url):
  try:
    response = urllib.request.urlopen(url,timeout=1.5)
    h = response.read()
    h = h.decode('utf-8')
    return h
  except: 
    print('Invalid url or time out')
    return None

def parse(html,url=None):
  soup = BeautifulSoup(html,"html.parser")
  out_urls = soup.find_all('a', href=True)
  out_urls_list = []
  for single_url in out_urls:
    href = single_url['href']
    if (bool(re.match('^https',href))):
      out_urls_list.append(href)
  
  title = None
  head = soup.head
  if soup.head is not None:
    title = soup.head.title
    if title is not None:
     title = title.text

  description = None
  if soup.head is not None:
    description = soup.head.find('meta',{"name" : "description"})
    if description is not None:
      description = description['content']
  
  text = soup.find('body')
  if text is not None:
    text = text.text
  x = {
     'url' :  url ,
    'title' :  title, 
     'description' : description,
     'text' : text, 
    'out_urls' : out_urls_list,

        }
  return x

def BFS(url,max_depth=2):
  url_dict = {url:0}
  url_info = {}
  url_list = [url]
  index = 0
  while len(url_list) != 0:
    target_url = url_list.pop(0)
    
    time.sleep(0.1)
    if url_dict[target_url] < max_depth+1:
      
      target_html = crawl(target_url)
      if target_html is not None:
        
        target_info = parse(target_html,target_url)
        url_info[index] =target_info
        index = index +1

        for url in target_info['out_urls']:
          if url not in url_dict:
            url_dict[url] = url_dict[target_url]+1
            url_list.append(url)
    

  return url_dict,url_info

def url_word_score(url_info):
  word_score_set = {}
  for single_info in url_info:
    word_score_dic = {}
    info = url_info[single_info]
    
    title_content = None
    if (info['title'] is not None):
      title_content = info['title'].lower()
      title_content = re.sub(r'[^a-zA-Z0-9]', ' ',title_content ).split()
      for title_word in title_content:
        if title_word not in word_score_dic.keys():
          word_score_dic[title_word] = 3
        else:
          word_score_dic[title_word] = word_score_dic[title_word]+ 3

    description_content = None
    if (info['description'] is not None):
      description_content = info['description'].lower()
      description_content = re.sub(r'[^a-zA-Z0-9]', ' ',description_content ).split()
      for description_word in description_content:
        if description_word not in word_score_dic.keys():
          word_score_dic[description_word] = 2
        else:
          word_score_dic[description_word] = word_score_dic[description_word]+ 2

    text_content = None
    if (info['text'] is not None):
      text_content = info['text'].lower()
      text_content = re.sub(r'[^a-zA-Z0-9]', ' ',text_content).split()
      for text_word in text_content:
        if text_word not in word_score_dic.keys():
          word_score_dic[text_word] = 1
        else:
          word_score_dic[text_word] = word_score_dic[text_word]+ 1
    
    word_score_set[single_info] = word_score_dic


  return word_score_set

#test
a,b = BFS(http,max_depth = 1)
c = url_word_score(b)
e = inverted_index(c)
x = search('as',e,b)

x

e = inverted_index(c)

f = [(value, key) for key,value in e['as'].items()]
g = heapq.nlargest(10,f)
h = [(key, value) for value,key in g]

print(e['as'])
print(f)
print(g)
print(h)

i={}
for hh in h:
  i[hh[0]] = hh[1]
  print(hh[0],hh[1])
print(i)

e['as']

def inverted_index(word_score_set):
  word_set = {}
  for url_int in word_score_set:
    single_set = word_score_set[url_int]
    for word in single_set:
      if word not in word_set.keys():
        word_set[word] = {}
        word_set[word][url_int] = single_set[word]
      else:
        word_set[word][url_int] = single_set[word]
  
  for single_word in word_set:
    m = min(10, len(word_set[single_word]))
    inverted_tuple = [(value, key) for key,value in word_set[single_word].items()]
    top_ten_tuple = heapq.nlargest(m,inverted_tuple)
    top_ten_tuple_inverted_back = [(key, value) for value,key in top_ten_tuple]
    #top_ten_dict = dict(top_ten_tuple_inverted_back)
    top_ten_dict= {}

    for single_tuple in top_ten_tuple_inverted_back:
      top_ten_dict[single_tuple[0]] = single_tuple[1]

    
    word_set[single_word] = top_ten_dict

  return word_set



def search(word,word_score_set,url_info):
  result_list = []
  url_score_dict = word_score_set[word]
  for url in url_score_dict:
    result_list.append(url_info[url])

  return result_list

x = search('as',e,b)

x[1]

heapq.nlargest(5,e['as'].items())

e['as']

dict((y,x) for x,y in e['as'].items())



# BFS (iteration) V.S DFS (recursion or iteration)
# Seed url
# Useful info 
# url
# title
# description
# text
# outgoing urls